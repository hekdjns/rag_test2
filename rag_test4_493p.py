import streamlit as st
from loguru import logger

from langchain_core.messages import ChatMessage
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langserve import RemoteRunnable


def simple_len(text: str) -> int:
    return len(text)


def get_text(docs):
    doc_list = []

    for doc in docs:
        file_name = doc.name
        with open(file_name, "wb") as file:
            file.write(doc.getvalue())
            logger.info(f"Uploaded: {file_name}")

        if file_name.lower().endswith(".pdf"):
            loader = PyPDFLoader(file_name)
        elif file_name.lower().endswith(".docx"):
            loader = Docx2txtLoader(file_name)
        else:
            logger.warning(f"Unsupported file: {file_name}")
            continue

        doc_list.extend(loader.load_and_split())
    return doc_list


def get_text_chunks(text):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=100,
        length_function=simple_len,
    )
    return splitter.split_documents(text)


def get_vectorstore(text_chunks):
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )
    return FAISS.from_documents(text_chunks, embeddings)


def main():
    st.set_page_config(page_title="RAG Test", page_icon="ğŸ“š")
    st.title("ğŸ“š _RAG Test 4_ â€” :red[Q/A Chat]")

    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    if "retriever" not in st.session_state:
        st.session_state["retriever"] = None
    if "processComplete" not in st.session_state:
        st.session_state["processComplete"] = False

    def add_history(role, content):
        st.session_state["messages"].append(ChatMessage(role=role, content=content))

    def print_history():
        for msg in st.session_state["messages"]:
            st.chat_message(msg.role).write(msg.content)

    with st.sidebar:
        uploaded = st.file_uploader(
            "íŒŒì¼ ì—…ë¡œë“œ", type=["pdf", "docx"], accept_multiple_files=True
        )
        process = st.button("ë¬¸ì„œ ì²˜ë¦¬")

    if process and uploaded:
        texts = get_text(uploaded)
        chunks = get_text_chunks(texts)
        vectordb = get_vectorstore(chunks)
        st.session_state["retriever"] = vectordb.as_retriever(
            search_type="mmr",
            verbose=True
        )
        st.session_state["processComplete"] = True

        add_history("assistant", "ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")

    if not st.session_state["messages"]:
        add_history("assistant", "ì•ˆë…•í•˜ì„¸ìš”! ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")

    print_history()

    user_input = st.chat_input("ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”")
    if user_input:
        add_history("user", user_input)
        st.chat_message("user").write(user_input)

        with st.chat_message("assistant"):
            llm = RemoteRunnable("https://ragtest.ngrok.app/llm/")
            stream_box = st.empty()

            if st.session_state["processComplete"]:
                prompt = ChatPromptTemplate.from_template(
                    """ë‹¹ì‹ ì€ ë™ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ì†Œí”„íŠ¸ì›¨ì–´ê³¼ ì•ˆë‚´ AI ì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— 30ì ì´ë‚´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.

Question: {question} 
Context: {context}
Answer:"""
                )
                retriever = st.session_state["retriever"]

                chain = (
                    {
                        "context": retriever | (lambda x: "\n\n".join(d.page_content for d in x)),
                        "question": RunnablePassthrough(),
                    }
                    | prompt
                    | llm
                    | StrOutputParser()
                )

                chunks = []
                for chunk in chain.stream(user_input):
                    chunks.append(chunk)
                    stream_box.markdown("".join(chunks))

                add_history("assistant", "".join(chunks))
            else:
                prompt = ChatPromptTemplate.from_template(
                    "ë‹¤ìŒ ì§ˆë¬¸ì— ê°„ë‹¨íˆ ë‹µë³€í•˜ì„¸ìš”:\n{input}"
                )
                chain = prompt | llm | StrOutputParser()

                chunks = []
                for chunk in chain.stream(user_input):
                    chunks.append(chunk)
                    stream_box.markdown("".join(chunks))

                add_history("assistant", "".join(chunks))


if __name__ == "__main__":
    main()
