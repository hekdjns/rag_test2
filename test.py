import streamlit as st
import tiktoken
from loguru import logger

from langchain_core.messages import ChatMessage

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import Docx2txtLoader
from langchain_community.document_loaders import UnstructuredPowerPointLoader

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langserve import RemoteRunnable


def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    return len(tokenizer.encode(text))


def get_text(docs):
    doc_list = []
    for doc in docs:
        file_name = doc.name
        with open(file_name, "wb") as f:
            f.write(doc.getvalue())
            logger.info(f"Uploaded {file_name}")

        if file_name.endswith(".pdf"):
            loader = PyPDFLoader(file_name)
        elif file_name.endswith(".docx"):
            loader = Docx2txtLoader(file_name)
        elif file_name.endswith(".pptx"):
            loader = UnstructuredPowerPointLoader(file_name)
        else:
            continue

        doc_list.extend(loader.load_and_split())
    return doc_list


def get_text_chunks(text):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=100,
        length_function=tiktoken_len,
    )
    return splitter.split_documents(text)


def get_vectorstore(chunks):
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )
    return FAISS.from_documents(chunks, embeddings)


def main():
    st.set_page_config(page_title="Remote RAG", page_icon="ğŸ“˜")
    st.title("ğŸ“˜ **Remote RAG Chatbot** (invoke mode)")

    # ì„¸ì…˜ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []

    if "processComplete" not in st.session_state:
        st.session_state.processComplete = False

    if "retriever" not in st.session_state:
        st.session_state.retriever = None

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        uploaded_files = st.file_uploader(
            "ğŸ“‚ ë¬¸ì„œ ì—…ë¡œë“œ",
            type=["pdf", "docx"],
            accept_multiple_files=True,
        )
        process = st.button("ğŸ“Œ Process")

    # ë¬¸ì„œ ì²˜ë¦¬
    if process:
        if uploaded_files:
            docs = get_text(uploaded_files)
            chunks = get_text_chunks(docs)
            vectorstore = get_vectorstore(chunks)
            st.session_state.retriever = vectorstore.as_retriever(
                search_type="mmr", vervose=True
            )
            st.session_state.processComplete = True
            st.success("âœ” ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            st.warning("ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”.")

    # ëŒ€í™” ê¸°ë¡ ì¶œë ¥
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    # ì‚¬ìš©ì ì…ë ¥
    user_input = st.chat_input("ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        st.chat_message("user").write(user_input)

        with st.chat_message("assistant"):
            llm = RemoteRunnable("https://ragtest.ngrok.app/llm/")

            # ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ (RAG)
            if st.session_state.processComplete:
                RAG_PROMPT_TEMPLATE = """
                ë‹¹ì‹ ì€ ë™ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ì†Œí”„íŠ¸ì›¨ì–´ê³¼ ì•ˆë‚´ AI ì…ë‹ˆë‹¤. 
                ê²€ìƒ‰ëœ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë§ëŠ” ë‹µë³€ì„ 30ë¬¸ì ì´ë‚´ë¡œ í•˜ì„¸ìš”.
                ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.

                Question: {question}
                Context: {context}
                Answer:
                """

                prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

                retriever = st.session_state.retriever

                rag_chain = (
                    {
                        "context": retriever | (lambda docs: "\n\n".join([d.page_content for d in docs])),
                        "question": RunnablePassthrough(),
                    }
                    | prompt
                    | llm
                )

                # ğŸ”¥ STREAM ê¸ˆì§€ â†’ invoke ì‚¬ìš©
                result = rag_chain.invoke(user_input)

            # ì¼ë°˜ ì§ˆë¬¸ ëª¨ë“œ
            else:
                prompt = ChatPromptTemplate.from_template(
                    "ë‹¤ìŒ ì§ˆë¬¸ì— ê°„ê²°íˆ ë‹µí•˜ì„¸ìš”:\n{input}"
                )
                chain = prompt | llm
                result = chain.invoke({"input": user_input})

            # LangServe invoke ê²°ê³¼ëŠ” {'output': "..."} í˜•íƒœì„
            answer = result["output"] if isinstance(result, dict) else str(result)

            st.session_state.messages.append({"role": "assistant", "content": answer})
            st.write(answer)


if __name__ == "__main__":
    main()
